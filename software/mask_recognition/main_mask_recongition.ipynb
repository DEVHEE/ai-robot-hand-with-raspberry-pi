{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "face_mask_detector.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Jupyter Notebook"
      ],
      "metadata": {
        "id": "RpuM4JQK-ecz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install the necessary packages."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!pip install tensorflow==2.4.3"
      ],
      "outputs": [],
      "metadata": {
        "id": "qdtSckfk-hua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download the necessary archive."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If you want to download only archive:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1XQCXaVy5YVoLPF9V2mR9MAGQXElbh-53' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1XQCXaVy5YVoLPF9V2mR9MAGQXElbh-53\" -O mask_archive_v0.3.zip && rm -rf ~/cookies.txt"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If you want to download archive with model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!wget --load-cookies ~/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies ~/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1b29DgQo821Spa4m0eplq0RlLCvQn1f74' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1b29DgQo821Spa4m0eplq0RlLCvQn1f74\" -O mask_archive_with_model_v0.3.zip && rm -rf ~/cookies.txt"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unzip archive and get directory."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If you want to unzip and get only archive:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip -qq mask_archive_v0.3.zip\n",
        "%cd mask_archive_v0.3"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### If you want to unzip and get archive with model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!unzip -qq mask_archive_with_model_v0.3.zip\n",
        "%cd mask_archive_with_model_v0.3"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Code"
      ],
      "metadata": {
        "id": "ZQl-4t1OKQ3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import modules."
      ],
      "metadata": {
        "id": "vU6IIjjeKUdb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Import modules.\n",
        "import os\n",
        "import cv2\n",
        "import imutils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from imutils import paths\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report"
      ],
      "outputs": [],
      "metadata": {
        "id": "t9g021RoKQNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function to display images in Jupyter Notebooks and Google Colab."
      ],
      "metadata": {
        "id": "hAakXkQL5ate"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def plt_imshow(title, image):\n",
        "    # Convert the image frame BGR to RGB color space and display it.\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(image)\n",
        "    plt.title(title)\n",
        "    plt.grid(False)\n",
        "    plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "2R7wEnD85YRd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing face mask recognizer training script with Keras and TensorFlow."
      ],
      "metadata": {
        "id": "Wegd_Pv58Jby"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def recognize_and_predict_mask(frame, faceNet, maskNet):\n",
        "    # Hold the dimensions of the frame and then create a block in the frame.\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        "\n",
        "    # Pass the blob through the network and acquire face recognition.\n",
        "    faceNet.setInput(blob)\n",
        "    recognitions = faceNet.forward()\n",
        "\n",
        "    # Initialize the face list, corresponding location, and prediction list of face mask networks.\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        "\n",
        "    # Loop over the recognitions.\n",
        "    for i in range(0, recognitions.shape[2]):\n",
        "        \n",
        "        # Extract the confidence (like probability) related to recognition.\n",
        "        confidence = recognitions[0, 0, i, 2]\n",
        "\n",
        "        # Filter weak recognition by checking whether the confidence is greater than the minimum confidence.\n",
        "        if confidence > args[\"confidence\"]:\n",
        "            \n",
        "            # Calculate the (x, y) coordinates of the bounding box for the object.\n",
        "            box = recognitions[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # Make sure the boundary boxes is within frame dimensions.\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "            # Extract the face ROI, convert it from BGR to RGB channel ordering, resize it to 224x224, and preprocess it.\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        "\n",
        "            # Add the face and bounding boxes to their respective lists.\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        "\n",
        "    # Only make a predictions if at least one face was recognized\n",
        "    if len(faces) > 0:\n",
        "        # For faster inference we'll make batch predictions on all faces at the same time rather than one-by-one predictions in the above `for` loop.\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "    # Return a 2-tuple of the face locations and their corresponding locations.\n",
        "    return (locs, preds)"
      ],
      "outputs": [],
      "metadata": {
        "id": "zbFBmMc45xE2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Parse the arguments.\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-d\", \"--dataset\", type=str, default=\"dataset\",\n",
        "#     help=\"the path of the input dataset\")\n",
        "# ap.add_argument(\"-p\", \"--plot\", type=str, default=\"loss_acc_plot.png\",\n",
        "#     help=\"the path of output loss/accuracy plot\")\n",
        "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "#     default=\"maskRecognizer.model\",\n",
        "#     help=\"the path to output the face mask recognizer model\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# Since we are using Jupyter Notebooks we can replace our argument parsing code with hard coded arguments and values.\n",
        "args = {\n",
        "    \"dataset\": \"dataset\",\n",
        "    \"plot\": \"loss_acc_plot.png\",\n",
        "    \"model\": \"mask_recognizer.model\"\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "6Yk-lRE3Klu3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Set init learning rate, epochs, and batch size.\n",
        "INIT_LR = 1e-4\n",
        "EPOCHS = 20\n",
        "BS = 32"
      ],
      "outputs": [],
      "metadata": {
        "id": "6ZtJLhnI4M4Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Get the image list from the dataset directory, and then initialize the data(images) and class image list.\n",
        "print(\"Loading images...\")\n",
        "imagePaths = list(paths.list_images(args[\"dataset\"]))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop over the image paths.\n",
        "for imagePath in imagePaths:\n",
        "    # Extract class labels from file names.\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "    # Load the 224x224 input image and preprocess it.\n",
        "    image = load_img(imagePath, target_size=(224, 224))\n",
        "    image = img_to_array(image)\n",
        "    image = preprocess_input(image)\n",
        "\n",
        "    # Update the data and label list, respectively.\n",
        "    data.append(image)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert data and labels to NumPy array.\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Perform one-hot encoding on the labels.\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "labels = to_categorical(labels)"
      ],
      "outputs": [],
      "metadata": {
        "id": "b0qzVmdtLBuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Partition the data into training and testing splits using 75% of the data for training and the remaining 25% for testing.\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "    test_size=0.20, stratify=labels, random_state=42)\n",
        "\n",
        "# Construct the training image generator for data augmentation.\n",
        "aug = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    zoom_range=0.15,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.15,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "m-yhgBfhLUkL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the MobileNetV2 network to ensure that the head FC layer set is left off.\n",
        "baseModel = MobileNetV2(weights=\"imagenet\", include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3)))\n",
        "\n",
        "# Construct the head of the model to be placed on top of the base model.\n",
        "headModel = baseModel.output\n",
        "headModel = AveragePooling2D(pool_size=(7, 7))(headModel)\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(128, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(2, activation=\"softmax\")(headModel)\n",
        "\n",
        "# Place the head FC model on top of the base model (it will be the actual model we will train).\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "# Repeat to all layers of the base model to fix it so that it is not updated during the first training process.\n",
        "for layer in baseModel.layers:\n",
        "    layer.trainable = False"
      ],
      "outputs": [],
      "metadata": {
        "id": "vn23s_cXMqWV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Compile our model.\n",
        "print(\"Compiling model...\")\n",
        "opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=opt,\n",
        "    metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the head of the network.\n",
        "print(\"Training head...\")\n",
        "H = model.fit(\n",
        "    aug.flow(trainX, trainY, batch_size=BS),\n",
        "    steps_per_epoch=len(trainX) // BS,\n",
        "    validation_data=(testX, testY),\n",
        "    validation_steps=len(testX) // BS,\n",
        "    epochs=EPOCHS)"
      ],
      "outputs": [],
      "metadata": {
        "id": "3JFn6PCCNBmm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Make predictions on the testing set\n",
        "print(\"Evaluating network...\")\n",
        "predIdxs = model.predict(testX, batch_size=BS)\n",
        "\n",
        "# For each image in the testing set we need to find the index of the label with corresponding largest predicted probability.\n",
        "predIdxs = np.argmax(predIdxs, axis=1)\n",
        "\n",
        "# Show a nicely formatted classification report.\n",
        "print(classification_report(testY.argmax(axis=1), predIdxs,\n",
        "    target_names=lb.classes_))\n",
        "\n",
        "# Serialize the model to disk.\n",
        "print(\"Saving mask recognizer model...\")\n",
        "model.save(args[\"model\"], save_format=\"h5\")\n",
        "\n",
        "# Make a plot  the training loss and accuracy.\n",
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Mask Recognition\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss / Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "N8xYyiJJNKil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing face mask recognizer for images with OpenCV."
      ],
      "metadata": {
        "id": "5ouojbu08ZAD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Parse the arguments.\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-i\", \"--image\", required=True,\n",
        "#     help=\"the path of input image\")\n",
        "# ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "#     default=\"face_recognizer\",\n",
        "#     help=\"the path of face recognizer model directory\")\n",
        "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "#     default=\"maskRecognizer.model\",\n",
        "#     help=\"the path of trained face mask recognizer model\")\n",
        "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "#     help=\"minimum probability to filter weak recognitions\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# Since we are using Jupyter Notebooks we can replace our argument parsing code with hard coded arguments and values.\n",
        "args = {\n",
        "    \"image\": \"assets/image/mask-1.png\",\n",
        "    \"face\": \"face_recognizer\",\n",
        "    \"model\": \"mask_recognizer.model\",\n",
        "    \"confidence\": 0.5\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "0UeTwvm4OqHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load our serialized face recognizer model from disk.\n",
        "print(\"Loading face recognizer model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "    \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "net = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# Load the face mask recognizer model from disk.\n",
        "print(\"Loading face mask recognizer model...\")\n",
        "model = load_model(args[\"model\"])"
      ],
      "outputs": [],
      "metadata": {
        "id": "hbIBe-J9PR0Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load the input image from disk, clone it, and grab the image spatial dimensions.\n",
        "image = cv2.imread(args[\"image\"])\n",
        "orig = image.copy()\n",
        "(h, w) = image.shape[:2]\n",
        "\n",
        "# Construct a blob from the image.\n",
        "blob = cv2.dnn.blobFromImage(image, 1.0, (300, 300),\n",
        "    (104.0, 177.0, 123.0))\n",
        "\n",
        "# Pass the blob through the network and obtain the face recognitions.\n",
        "print(\"Computing face recognitions...\")\n",
        "net.setInput(blob)\n",
        "recognitions = net.forward()"
      ],
      "outputs": [],
      "metadata": {
        "id": "w9kjaZJIPhMJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Loop over the recognitions.\n",
        "for i in range(0, recognitions.shape[2]):\n",
        "    # Extract the confidence (i.e., probability) associated with the recognition.\n",
        "    confidence = recognitions[0, 0, i, 2]\n",
        "\n",
        "    # Filter out weak recognitions by ensuring the confidence is greater than the minimum confidence.\n",
        "    if confidence > args[\"confidence\"]:\n",
        "        # Compute the (x, y)-coordinates of the bounding box for the object.\n",
        "        box = recognitions[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "        # Ensure the bounding boxes fall within the dimensions of the frame.\n",
        "        (startX, startY) = (max(0, startX), max(0, startY))\n",
        "        (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "        ## Extract the face ROI, convert it from BGR to RGB channel ordering, resize it to 224x224, and preprocess it.\n",
        "        face = image[startY:endY, startX:endX]\n",
        "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "        face = cv2.resize(face, (224, 224))\n",
        "        face = img_to_array(face)\n",
        "        face = preprocess_input(face)\n",
        "        face = np.expand_dims(face, axis=0)\n",
        "\n",
        "        # Pass the face through the model to determine if the face has a mask or not.\n",
        "        (mask, withoutMask) = model.predict(face)[0]\n",
        "\n",
        "        # Determine the class label and color we'll use to draw the bounding box and text.\n",
        "        label = \"MASK\" if mask > withoutMask else \"NO MASK\"\n",
        "        color = (0, 255, 0) if label == \"MASK\" else (0, 0, 255)\n",
        "\n",
        "        # Include the probability in the label.\n",
        "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "        # Display the label and bounding box rectangle on the output frame.\n",
        "        cv2.putText(image, label, (startX, startY - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "        cv2.rectangle(image, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "# Show the output image.\n",
        "plt_imshow(\"MASK RECOGNITION RESULT\", image)"
      ],
      "outputs": [],
      "metadata": {
        "id": "jhYecZ-wP8bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing face mask recognizer in real-time video streams with OpenCV."
      ],
      "metadata": {
        "id": "GjLz2yzZSW7D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "def recognize_and_predict_mask(frame, faceNet, maskNet):\n",
        "    # Hold the dimensions of the frame and then create a block in the frame.\n",
        "    (h, w) = frame.shape[:2]\n",
        "    blob = cv2.dnn.blobFromImage(frame, 1.0, (300, 300),\n",
        "        (104.0, 177.0, 123.0))\n",
        "\n",
        "    # Pass the blob through the network and acquire face recognition.\n",
        "    faceNet.setInput(blob)\n",
        "    recognitions = faceNet.forward()\n",
        "\n",
        "    # Initialize the face list, corresponding location, and prediction list of face mask networks.\n",
        "    faces = []\n",
        "    locs = []\n",
        "    preds = []\n",
        "\n",
        "    # Loop over the recognitions.\n",
        "    for i in range(0, recognitions.shape[2]):\n",
        "        # Extract the confidence (like probability) related to recognition.\n",
        "        confidence = recognitions[0, 0, i, 2]\n",
        "\n",
        "        # Filter weak recognition by checking whether the confidence is greater than the minimum confidence.\n",
        "        if confidence > args[\"confidence\"]:\n",
        "            # Calculate the (x, y) coordinates of the bounding box for the object.\n",
        "            box = recognitions[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # Make sure the boundary boxes is within frame dimensions.\n",
        "            (startX, startY) = (max(0, startX), max(0, startY))\n",
        "            (endX, endY) = (min(w - 1, endX), min(h - 1, endY))\n",
        "\n",
        "            # Extract the face ROI, convert it from BGR to RGB channel ordering, resize it to 224x224, and preprocess it.\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
        "            face = cv2.resize(face, (224, 224))\n",
        "            face = img_to_array(face)\n",
        "            face = preprocess_input(face)\n",
        "\n",
        "            # Add the face and bounding boxes to their respective lists.\n",
        "            faces.append(face)\n",
        "            locs.append((startX, startY, endX, endY))\n",
        "\n",
        "    # Only make a predictions if at least one face was recognized.\n",
        "    if len(faces) > 0:\n",
        "        # For faster inference we'll make batch predictions on all faces at the same time rather than one-by-one predictions in the above `for` loop.\n",
        "        faces = np.array(faces, dtype=\"float32\")\n",
        "        preds = maskNet.predict(faces, batch_size=32)\n",
        "\n",
        "    # Return a 2-tuple of the face locations and their corresponding locations.\n",
        "    return (locs, preds)"
      ],
      "outputs": [],
      "metadata": {
        "id": "oSABWE7GGTlp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# # construct the argument parser and parse the arguments\n",
        "# ap = argparse.ArgumentParser()\n",
        "# ap.add_argument(\"-f\", \"--face\", type=str,\n",
        "# \tdefault=\"face_recognizer\",\n",
        "# \thelp=\"path to face recognizer model directory\")\n",
        "# ap.add_argument(\"-m\", \"--model\", type=str,\n",
        "# \tdefault=\"mask_recognizer.model\",\n",
        "# \thelp=\"path to trained face mask recognizer model\")\n",
        "# ap.add_argument(\"-c\", \"--confidence\", type=float, default=0.5,\n",
        "# \thelp=\"minimum probability to filter weak recognitions\")\n",
        "# args = vars(ap.parse_args())\n",
        "\n",
        "# Since we are using Jupyter Notebooks we can replace our argument parsing code with hard coded arguments and values.\n",
        "args = {\n",
        "    \"input\": \"assets/video/CDC_mask_720.mp4\",\n",
        "    \"output\": \"mask_recognize_output.avi\",\n",
        "    \"face\": \"face_recognizer\",\n",
        "    \"model\": \"mask_recognizer.model\",\n",
        "    \"confidence\": 0.5\n",
        "}"
      ],
      "outputs": [],
      "metadata": {
        "id": "RQGWgOleHABs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Load our serialized face recognizer model from disk.\n",
        "print(\"Loading face recognizer model...\")\n",
        "prototxtPath = os.path.sep.join([args[\"face\"], \"deploy.prototxt\"])\n",
        "weightsPath = os.path.sep.join([args[\"face\"],\n",
        "    \"res10_300x300_ssd_iter_140000.caffemodel\"])\n",
        "faceNet = cv2.dnn.readNet(prototxtPath, weightsPath)\n",
        "\n",
        "# Load the face mask recognizer model from disk.\n",
        "print(\"Loading face mask recognizer model...\")\n",
        "maskNet = load_model(args[\"model\"])\n",
        "\n",
        "# Grab a reference to the video file and initialize pointer to output video file.\n",
        "print(\"Opening video file...\")\n",
        "vs = cv2.VideoCapture(args[\"input\"])\n",
        "writer = None"
      ],
      "outputs": [],
      "metadata": {
        "id": "X11wwP_WNmGQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Loop over the frames from the video stream.\n",
        "while True:\n",
        "    # Grab the next frame.\n",
        "    frame = vs.read()[1]\n",
        "\n",
        "    # If we did not grab a frame then we have reached the end of the video.\n",
        "    if frame is None:\n",
        "        break\n",
        "\n",
        "    # Resize the frame to have a maximum width of 400 pixels.\n",
        "    frame = imutils.resize(frame, width=400)\n",
        "\n",
        "    # Recognize faces in the frame and determine if they are wearing a face mask or not.\n",
        "    (locs, preds) = recognize_and_predict_mask(frame, faceNet, maskNet)\n",
        "\n",
        "    # Loop over the recognized face locations and their corresponding locations.\n",
        "    for (box, pred) in zip(locs, preds):\n",
        "        # Unpack the bounding box and predictions.\n",
        "        (startX, startY, endX, endY) = box\n",
        "        (mask, withoutMask) = pred\n",
        "\n",
        "        # Determine the class label and color we'll use to draw the bounding box and text.\n",
        "        label = \"MASK\" if mask > withoutMask else \"NO MASK\"\n",
        "        color = (0, 255, 0) if label == \"MASK\" else (0, 0, 255)\n",
        "\n",
        "        # Include the probability in the label.\n",
        "        label = \"{}: {:.2f}%\".format(label, max(mask, withoutMask) * 100)\n",
        "\n",
        "        # Display the label and bounding box rectangle on the output frame.\n",
        "        cv2.putText(frame, label, (startX, startY - 10),\n",
        "            cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 2)\n",
        "        cv2.rectangle(frame, (startX, startY), (endX, endY), color, 2)\n",
        "\n",
        "    # If the video writer is None and we are supposed to write the output video to disk initialize the writer.\n",
        "    if writer is None and args[\"output\"] is not None:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "        writer = cv2.VideoWriter(args[\"output\"], fourcc, 20,\n",
        "            (frame.shape[1], frame.shape[0]), True)\n",
        "\n",
        "    # If the writer is not None, write the frame to disk.\n",
        "    if writer is not None:\n",
        "        writer.write(frame)\n",
        "\n",
        "# Do a bit of cleanup.\n",
        "vs.release()\n",
        "\n",
        "# Check to see if the video writer point needs to be released.\n",
        "if writer is not None:\n",
        "    writer.release()"
      ],
      "outputs": [],
      "metadata": {
        "id": "Vevbsx5VNp2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested to view the video within Google Colab just execute the following code blocks.\n",
        "\n",
        "Our output video is produced in `.avi` format. First, we need to convert it to `.mp4` format."
      ],
      "metadata": {
        "id": "VpSXMhHhHOJw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "!ffmpeg -i mask_recognize_output.avi mask_recognize_output.mp4"
      ],
      "outputs": [],
      "metadata": {
        "id": "lhYbtMwnHTp-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Display video inline.\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(\"mask_recognize_output.mp4\", \"rb\").read()\n",
        "dataURL = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "    <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % dataURL)"
      ],
      "outputs": [],
      "metadata": {
        "id": "UJCPzNUbHk8n",
        "cellView": "form"
      }
    }
  ]
}